{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    ## this is for each document\n",
    "    def __init__(self):\n",
    "        self.inittoken_list = []\n",
    "        self.http_dic = {\"ALL\": []}\n",
    "        self.number_removed_list = []\n",
    "        self.number_dic = {\"ALL\":[]} ## recorded in init order\n",
    "        self.stemmed_list = []\n",
    "        self.punctuation_list = [\".\", \"'\", '\"', \"?\", \",\", \")\", \"(\", \"@\", \"%\", \"$\", \"*\", \n",
    "                                 \"-\", \"_\", \"/\", \"!\", \"#\", \"^\", \"&\", \"`\", \":\", \";\"]\n",
    "        self.poter = PorterStemmer()\n",
    "        self.stopward_list = [] \n",
    "        \n",
    "        self.stopward_dic = {\"ALL\":[]}\n",
    "        init_stopward_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "                              'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "                              'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "                              'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', \n",
    "                              'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', \n",
    "                              'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "                              'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', \n",
    "                              'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", \n",
    "                              'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \n",
    "                              \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "        self.stopward_adding( init_stopward_list)\n",
    "        self.stopwarded_list = []\n",
    "        self.word_dic = {} ## format:{term:[index_list]}\n",
    "        return None\n",
    "    \n",
    "    def read_file(self, storage_place):\n",
    "        #print(1)\n",
    "        if len(self.inittoken_list) != 0:\n",
    "            return \"you have already put some data in here\"\n",
    "        ## vertify type of input\n",
    "        if not isinstance(storage_place, str):\n",
    "            print(\"you should input where you store your document in string type.\")\n",
    "            return False\n",
    "        ## make document in to a list of list of strings, seperated in lines\n",
    "        storage_place = storage_place.strip(\"/\")\n",
    "        document_list = open(storage_place, 'rt').readlines()\n",
    "\n",
    "        ## make document into a single list of string\n",
    "        valid_index = 0\n",
    "        for line in document_list:\n",
    "            start_flag = 0\n",
    "            for stop_flag in range( len(line)):\n",
    "                valid_flag = False\n",
    "                if line[ stop_flag] == ' ' :\n",
    "                    word = line[ start_flag : stop_flag]\n",
    "                    valid_flag = self.preprocess_word(word, valid_index)\n",
    "                    start_flag = stop_flag + 1\n",
    "                if line[-1] == '.' and stop_flag == len(line)-1: \n",
    "                    # check the last word for each line\n",
    "                    word = line[ start_flag : -1]\n",
    "                    valid_flag = self.preprocess_word(word, valid_index)\n",
    "\n",
    "                if valid_flag:\n",
    "                    ## flag is true if word is valid\n",
    "                    valid_index = valid_index + 1\n",
    "                    \n",
    "        return True\n",
    "        \n",
    "    def preprocess_word(self, word, valid_index): \n",
    "        ## 應在這裡把 字串list、字典 建好\n",
    "        if self.http_remove(word):\n",
    "            return False\n",
    "        if self.number_remove(word):\n",
    "            return False\n",
    "        #self.minus_split( voca_index)\n",
    "        pun_removed = self.punctuation_remove(word)\n",
    "        if self.len_filter(pun_removed):\n",
    "            return False\n",
    "        stemmed =  self.stemming(pun_removed)\n",
    "        if self.stopwording(stemmed):\n",
    "            return False\n",
    "        self.word_dic_create(stemmed, valid_index)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def http_remove(self, word):\n",
    "        ## true if this word is a website address, and add it into http_dic \n",
    "        flag = 0\n",
    "        \n",
    "        if \"http\" == word[:4] or \"www\" == word[: 3]:\n",
    "        ## first 4 chars in word == http, or first 3 chars in word == www\n",
    "            flag = 1\n",
    "        \n",
    "        self.http_index = 0\n",
    "\n",
    "        if flag:\n",
    "            if not word in self.http_dic:\n",
    "                self.http_dic[\"ALL\"].append(word)\n",
    "                self.http_dic[ word] = []\n",
    "            self.http_dic[ word].append(self.http_index)\n",
    "            self.http_index = self.http_index + 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def number_remove(self, word):\n",
    "        ## true if 're is number in the word, and add it into number_dic \n",
    "        flag = 0\n",
    "        for char in word:\n",
    "            if ord(char) < 58 and ord(char) > 47:\n",
    "            ## ASCII for numbers : 48~57\n",
    "                flag = 1\n",
    "                break\n",
    "        self.number_index = 0\n",
    "        if flag:\n",
    "            if not word in self.number_dic:\n",
    "                self.number_dic[\"ALL\"].append(word)\n",
    "                self.number_dic[ word] = []\n",
    "            self.number_dic[ word].append(self.number_index)\n",
    "            self.number_index = self.number_index + 1\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def punctuation_remove(self, word):\n",
    "        for pun in self.punctuation_list:\n",
    "            if pun in word :\n",
    "                word = word.replace(pun, '')\n",
    "        return word\n",
    "    \n",
    "    def len_filter(self, word):\n",
    "        if len(word) < 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def stemming(self, word):\n",
    "        stemmed = self.poter.stem( word)\n",
    "        return stemmed\n",
    "    \n",
    "    def stopwording(self, word):\n",
    "        ## true if the word is stopword\n",
    "        if word in self.stopward_dic:\n",
    "            #self.stopward_dic['ALL'].append( voca_index)\n",
    "            #self.stopward_dic[ dest_document[voca_index]].append(voca_index)\n",
    "            return True\n",
    "\n",
    "        ## add normal word into stopworded_list\n",
    "        self.stopwarded_list.append(word)\n",
    "        return False\n",
    "        \n",
    "    def word_dic_create(self, word, index):\n",
    "        if word in self.word_dic:\n",
    "            self.word_dic[word].append(index)\n",
    "        else:\n",
    "            self.word_dic[word] = [index]\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def minus_split(self):####### INCOMPLEPE #######\n",
    "        for voca_index in  range( len( self.inittoken_list)):        \n",
    "            if \"-\" in self.inittoken_list[voca_index]:\n",
    "                temp = self.inittoken_list[voca_index].split(\"-\")\n",
    "                self.inittoken_list.append(temp)\n",
    "                self.inittoken_list[voca_index] = self.inittoken_list[voca_index].replace(\"-\", \"\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def stopward_adding(self, new_ward_list):\n",
    "        ## check for type of list\n",
    "        if not isinstance(new_ward_list, list):\n",
    "            print(\"want a list. in stopward_adding\")\n",
    "            return False\n",
    "        for stopward in new_ward_list:\n",
    "            ## check for type of each ward in list\n",
    "            if not isinstance(stopward, str):\n",
    "                print(\"want a list of string. in stopward_adding\")\n",
    "                return False\n",
    "            ## stem and add \n",
    "            stemmed_stopward = self.poter.stem( stopward)\n",
    "            if not stemmed_stopward in self.stopward_dic:\n",
    "                self.stopward_list.append( stemmed_stopward)\n",
    "                self.stopward_dic.update({stemmed_stopward: []})\n",
    "        #self.stopward_flag[0] = self.stopward_flag[0] +1\n",
    "        return 0\n",
    "    \n",
    "    def punctuation_adding(self, new_pun):\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def save_result(self):\n",
    "        with open(\"R09725049_result.txt\" , \"w\") as text_file:\n",
    "            text_file.write(str(self.stopwarded_list))\n",
    "        return \"file saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Dictionary():\n",
    "    def __init__(self):\n",
    "        self.preprocess_list = []\n",
    "        return None\n",
    "    \n",
    "    def preprocess_all_file(self, pre_path):\n",
    "        if not isinstance(pre_path, str):\n",
    "            print(\"you should input where you store your document in string type.\")\n",
    "            return False\n",
    "        ## make document in to a list of list of strings, seperated in lines\n",
    "        pre_path = pre_path.strip(\"/\")\n",
    "        ## \n",
    "        all_file_list = os.listdir(pre_path)\n",
    "        docu_index = 0\n",
    "        for document in range(len(all_file_list)):\n",
    "            self.preprocess_list.append(Preprocess())\n",
    "            self.preprocess_list[docu_index].read_file(pre_path + \"/\" +  str(document +1) + \".txt\")\n",
    "            docu_index = docu_index + 1\n",
    "        return None\n",
    "    \n",
    "    def document_frequency(self):\n",
    "        ### .isin() is too slow, so we use .in() with dic instead\n",
    "        ## 1. construct a dictionary and DataFrame of terms\n",
    "        ## 2. sort the term of the DataFrame in ascending order, reorder the index in dictionary\n",
    "        ## 3. append t_index into DataFrame\n",
    "        \n",
    "        self.term_dic = {}\n",
    "        self.docu_freq_df = pd.DataFrame(columns = [\"term\", \"df\", \"exsist_docu_list\"])\n",
    "        self.docu_freq_df.astype({'df': 'int32'}).dtypes\n",
    "        \n",
    "        ## 1\n",
    "        docu_index = 0\n",
    "        term_index = 0\n",
    "        for document in self.preprocess_list:\n",
    "            #print(\"--\")\n",
    "            #print(\"docu\", docu_index)\n",
    "            for term in document.word_dic:\n",
    "                #print(term, docu_index)\n",
    "                ## put term into term_dic, and append it into docu_freq_df\n",
    "                \n",
    "                if term in self.term_dic:\n",
    "                    ## real index is in term_index_list[0], which .to_list() return a list\n",
    "                    #print(term, i)\n",
    "                    ## raise df by 1, and append docu_index into exsist_docu_list\n",
    "                    self.docu_freq_df.at[ self.term_dic[term], \"df\"] += 1 \n",
    "                    self.docu_freq_df.at[ self.term_dic[term], \"exsist_docu_list\"].append(docu_index) \n",
    "                    \n",
    "                else:\n",
    "                    self.docu_freq_df = self.docu_freq_df.append(pd.DataFrame([[term, 1, [docu_index]]], \n",
    "                                                                              columns = [\"term\", \"df\", \"exsist_docu_list\"]), \n",
    "                                                                 ignore_index=True)\n",
    "                    self.term_dic[term] = term_index ## this value will be index of term in dataframe\n",
    "                    term_index += 1\n",
    "            docu_index += 1 \n",
    "        \n",
    "        ## 2\n",
    "        self.docu_freq_df = self.docu_freq_df.sort_values(by=['term']).reset_index().drop([\"index\"], axis= 1)\n",
    "        for index in range(self.docu_freq_df.shape[0]):\n",
    "            self.term_dic[self.docu_freq_df.loc[index, \"term\"]] = index\n",
    "        \n",
    "        ## 3\n",
    "        t_index = pd.DataFrame( np.arange(self.docu_freq_df.shape[0]), columns=[\"t_index\"])\n",
    "        self.docu_freq_df = pd.concat([t_index, self.docu_freq_df], axis = 1)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_document_frequency(self):\n",
    "        with open(\"dictionary.txt\" , \"w\") as text_file:\n",
    "            text_file.write(self.docu_freq_df.loc[:,\"t_index\": \"df\"].to_string(index= False))\n",
    "            ## output only \"t_index\", \"term\", \"df\" column\n",
    "        return \"file saved\"\n",
    "    \n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    #########\n",
    "    def docu_tf_idf(self):\n",
    "        ## ndarray format: [t_index, tf_idf]\n",
    "        self.docu_tf_idf_list = []\n",
    "        #flag = 1\n",
    "        for docu_index in range(len(self.preprocess_list)):\n",
    "            \n",
    "            first = 1\n",
    "            for term in self.preprocess_list[docu_index].word_dic :\n",
    "                if first:\n",
    "                    #print(self.term_dic[term], \":\", type(self.term_dic[term]))\n",
    "                    term_nparray = np.array([[int(self.term_dic[term]), self.tf_idf(term, docu_index)]])                    \n",
    "                    first = 0\n",
    "                else:\n",
    "                    term_nparray = np.append(term_nparray, [[int(self.term_dic[term]), self.tf_idf(term, docu_index)]], axis=0)\n",
    "            #term_nparray[:, 0] = np.round(term_nparray[:, 0], 0)\n",
    "            #if flag:\n",
    "            #    print(term_nparray)\n",
    "            term_nparray = term_nparray[np.argsort(term_nparray[:, 0])]\n",
    "            #if flag:\n",
    "            #    print(term_nparray)\n",
    "            #if flag:\n",
    "            #    flag = 0\n",
    "            #    print(term_nparray)\n",
    "            self.docu_tf_idf_list.append(term_nparray)\n",
    "        return None\n",
    "    \n",
    "    def tf_idf(self, term, docu_index):\n",
    "        tf = self.term_frequency(term, docu_index)\n",
    "        idf = self.inverse_document_frequency(term)\n",
    "        tf_idf = tf * idf\n",
    "        #print(tf_idf)\n",
    "        return tf_idf\n",
    "    \n",
    "    def term_frequency(self, term, docu_index):\n",
    "        #print(self.preprocess_list[docu_index].word_dic[term])\n",
    "        #print(\"tf = \", len(self.preprocess_list[docu_index].word_dic[term]))\n",
    "        tf = len(self.preprocess_list[docu_index].word_dic[term])\n",
    "        return tf\n",
    "    \n",
    "    def inverse_document_frequency(self, term):\n",
    "        term_index = self.term_dic[term]\n",
    "        idf = np.log10(len(self.preprocess_list) / self.docu_freq_df.loc[term_index, \"df\"])\n",
    "        #print(\"idf = \", idf)\n",
    "        return idf\n",
    "    \n",
    "    def save_tf_idf(self, docu_index):\n",
    "        head = np.array([[self.docu_tf_idf_list[docu_index].shape[0] #F\n",
    "                          , \"\"], [\"t_index\", \"tf_idf\"]])\n",
    "        #print(self.docu_tf_idf_list[docu_index][:, 0].astype(\"int\"))\n",
    "        whole = np.concatenate((head, self.docu_tf_idf_list[docu_index]), axis = 0)\n",
    "        #docu_index = str(docu_index)\n",
    "        #print(whole)\n",
    "        np.savetxt(str(docu_index) + '.txt', whole, delimiter='\\t', fmt = \"%s\")\n",
    "        #np.savetxt(str(docu_index) + '.txt', self.docu_tf_idf_list[docu_index], delimiter='\\t', fmt = \"%s\")\n",
    "        return \"file saved\"\n",
    "    \n",
    "    ############\n",
    "    ############\n",
    "    ############\n",
    "    ############\n",
    "    ############\n",
    "    def tf_idf_matrix_full(self):\n",
    "        self.tf_idf_matrix = np.zeros(shape = (self.docu_freq_df.shape[0] , len(self.preprocess_list)))\n",
    "        ## shape: [term_i, docu_i], init by 0\n",
    "        for docu_index in range(len(self.preprocess_list)):\n",
    "            for index in range(self.docu_tf_idf_list[docu_index].shape[0]):\n",
    "                term_index = int(self.docu_tf_idf_list[docu_index][index, 0])\n",
    "                #print(type(term_index))\n",
    "                self.tf_idf_matrix[term_index, docu_index] = self.docu_tf_idf_list[docu_index][index, 1]\n",
    "        return None\n",
    "    \n",
    "    def cos_similarity(self, docu_index_1, docu_index_2):\n",
    "        docu_1_vec = self.tf_idf_matrix[:, docu_index_1]\n",
    "        docu_2_vec = self.tf_idf_matrix[:, docu_index_2]\n",
    "        # Dot and norm\n",
    "        dot = sum(a*b for a, b in zip(docu_1_vec, docu_2_vec))\n",
    "        norm_a = sum(a*a for a in docu_1_vec) ** 0.5\n",
    "        norm_b = sum(b*b for b in docu_2_vec) ** 0.5\n",
    "        # Cosine similarity\n",
    "        similarity = dot / (norm_a*norm_b)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2 = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2.preprocess_all_file(\"D:\\Desktop\\IR\\PA2\\IRTM\")\n",
    "#pa2.preprocess_all_file(\"D:\\Desktop\\IR\\PA2\\\\temp\")  ## 測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file saved'"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa2.document_frequency()\n",
    "pa2.save_document_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2.docu_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa2.tf_idf_matrix_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04520061058883258\n",
      "0.042926246705518444\n",
      "0.19983854846589438\n"
     ]
    }
   ],
   "source": [
    "print(pa2.cos_similarity(0, 207))\n",
    "print(pa2.cos_similarity(651, 762))\n",
    "print(pa2.cos_similarity(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.array to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
